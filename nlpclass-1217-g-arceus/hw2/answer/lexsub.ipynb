{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrofitting Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# read words from lexicon\n",
    "def read_word_vector(filename):\n",
    "    wordVec={}\n",
    "    for line in open(filename, 'r'):\n",
    "        wordC = line.strip().lower().split()\n",
    "        temp = [w for w in wordC[1:]]\n",
    "        if wordC[0] not in wordVec:\n",
    "            wordVec[wordC[0]] = temp\n",
    "        else:\n",
    "            wordVec[wordC[0]] += temp\n",
    "    return wordVec\n",
    "path = os.path.dirname(os.getcwd())\n",
    "pathname = os.path.join(path,'data/lexicons','ppdb-xl.txt')             \n",
    "lexicon = read_word_vector(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofit(wordVecs, lexicon, T=10, alpha=1, beta=1):\n",
    "    newWordVecs = {}\n",
    "    wvVocab = set()\n",
    "    for word, it in wordVecs:\n",
    "        wvVocab.add(word)\n",
    "        newWordVecs[word] = it\n",
    "    for i in range(T):\n",
    "        for j, word in enumerate(wvVocab):\n",
    "            temp = np.zeros(newWordVecs[word].shape)\n",
    "            if word in lexicon:\n",
    "                count = 0\n",
    "                LoopVocab = lexicon[word]\n",
    "                for w in LoopVocab:\n",
    "                    if w in newWordVecs:\n",
    "                        temp += beta * newWordVecs[w]\n",
    "                        count += 1\n",
    "                newWordVecs[word] = ((temp + (alpha * wordVecs.query(word)))) / (count + alpha)\n",
    "    return newWordVecs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymagnitude.Magnitude at 0x7fabfffe2520>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymagnitude\n",
    "from pymagnitude import converter\n",
    "import numpy as np\n",
    "from gensim import scripts\n",
    "import optparse, tqdm\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "def convert_to_magnitude_and_retrofit(mag_path):\n",
    "    \n",
    "#     path = os.path.dirname(os.getcwd())\n",
    "#     print(mag_path)\n",
    "    path = os.getcwd()\n",
    "    if os.getcwd().split('/')[-1] == 'answer':\n",
    "        path = os.path.dirname(os.getcwd())\n",
    "    LL_path = os.path.join(path,'data/lexicons','ppdb-xl.txt')\n",
    "#     print(\"word vector reading.\\n\")\n",
    "    lexicon = read_word_vector(LL_path)\n",
    "    \n",
    "#     print(\"word vector read.\\n\")\n",
    "    wv = pymagnitude.Magnitude(mag_path)\n",
    "#     print(\"retrofitting.\\n\")\n",
    "    lexicon_retrofitted = retrofit(wv, lexicon)\n",
    "    \n",
    "#     print(\"retrofitted.\\n\")\n",
    "\n",
    "    retrofitpath = os.path.join(path,'data','glove.6B.100d.retrofit.txt')\n",
    "    if os.path.exists(os.path.join(path,'data','glove.6B.100d.retrofit.magnitude')):\n",
    "        wvecs=pymagnitude.Magnitude(os.path.join(path,'data','glove.6B.100d.retrofit.magnitude'))\n",
    "#         print(\"retrofit.magnitude exists\\n\")\n",
    "    else:\n",
    "#         print(\"retrofit.magnitude does/ not exist\\n\")\n",
    "        with open(retrofitpath, 'w') as f:\n",
    "            for word, embedding in lexicon_retrofitted.items():\n",
    "                s = word\n",
    "                for num in embedding:\n",
    "                    s += \" \" + str(num)\n",
    "                s += '\\n'\n",
    "                f.write(s)\n",
    "            target_file=os.path.join(path,'data','glove.6B.100d.retrofit.magnitude')\n",
    "\n",
    "            # target_file=os.path.join(path,'data',mag_path)\n",
    "            converter.convert(retrofitpath, target_file)\n",
    "#         print(os.path.join(path,'data','glove.6B.100d.retrofit.magnitude'))\n",
    "        wvecs = pymagnitude.Magnitude(os.path.join(path,'data','glove.6B.100d.retrofit.magnitude'))\n",
    "#         print(\"retrofit.magnitude exists\\n\")\n",
    "#     wvecs = pymagnitude.Magnitude(os.path.join(path,mag_path))\n",
    "    return wvecs\n",
    "mag_path = os.path.join(os.path.dirname(os.getcwd()), 'data','glove.6B.100d.retrofit.magnitude')\n",
    "convert_to_magnitude_and_retrofit(mag_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "Score=52.85\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "from lexsub import *\n",
    "import os\n",
    "lexSub=LexSub(os.path.join(os.path.dirname(os.getcwd()), 'data','glove.6B.100d.retrofit.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexSub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))\n",
    "\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
