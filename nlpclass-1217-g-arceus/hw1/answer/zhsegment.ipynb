{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签 订 高 科 技 合 作 协 议\n",
      "新 华 社 上 海 八 月 三 十 一 日 电 （ 记 者 白 国 良 、 夏 儒 阁 ）\n",
      "“ 中 美 合 作 高 科 技 项 目 签 字 仪 式 ” 今 天 在 上 海 举 行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.27\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "## Implementation of Entry class\n",
    "\n",
    "We defined a `class Entry` inside the `class Segment` that consists four components: `word`, `start_pos`, `log_prob`, `back_pointer` for iterating with the dynamic programming table `chart`.\n",
    "\n",
    "Within the `class Entry`, we also implemented the `__eq__` method to compare instances of the class. If the mismatches the components or is not an isntance of the class, `__eq__` method will return false.\n",
    "\n",
    "The `__lt__` and `__gt__` are also implemented for comparing the `log_prob` component. Since we want to push higher probability to the front of the heap, the operator signs were assigned reversely due to functionality of Python's heap queue algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entry:\n",
    "    def __init__(self,word,start_pos,log_prob,back_pointer):\n",
    "        self.word = word\n",
    "        self.start_pos = start_pos\n",
    "        self.log_prob = log_prob\n",
    "        self.back_pointer = back_pointer\n",
    "    \n",
    "    def __eq__(self,other):\n",
    "        if (self.word == other.word and self.start_pos == other.start_pos and self.log_prob == other.log_prob):\n",
    "            return True\n",
    "        if (not isinstance(other, type(self))):\n",
    "            return False\n",
    "        else:\n",
    "            False\n",
    "        \n",
    "    def __lt__(self,other):\n",
    "        return self.log_prob > other.log_prob\n",
    "        \n",
    "    def __gt__(self,other):\n",
    "        return self.log_prob <= other.log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Implementation of Iterative Segmenter Algorithm\n",
    "---\n",
    "We added the combination of unigram and bigram iterative algorithm (`segment_iter`) to `class Segment`.\n",
    "\n",
    "\n",
    "### Segmenter Algorithm with Unigram Probability\n",
    "\n",
    "The code below is the initial iterative Segmenter algorithm in unigram approach. Considering probability for long unseen words, we performed additive smoothing on the unigram probability model by modifying `self.missingfn` in `class Pdist`, and the overall dev score of the unigram approach reached $0.93$. The code for unigram's `Pdist` is included at the bottom of the code segment, where `self.missingfn` is changed from:\n",
    "\n",
    "```\n",
    "self.missingfn = missingfn or (lambda k, N: 1./ N)\n",
    "```\n",
    "to\n",
    "```\n",
    "self.missingfn = missingfn or (lambda k, N: 1./(N*1100**len(k)))\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram approach\n",
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    def segment(self, text):\n",
    "        ## Initialize the heap ##\n",
    "        heap = []\n",
    "        chart = {}\n",
    "        # for each word that matches input at pos 0, insert entry into heap\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            word = text[0:i+1]\n",
    "            chart[i] = Entry(None, None, None, None)\n",
    "            if word in self.Pw:\n",
    "                heapq.heappush(heap, Entry(word, 0, log10(self.Pw(word)), None))    ## heapq.heappush pushes the entry item to the heap queue\n",
    "        \n",
    "        ## Iteratively fill in chart[i] for all i ##\n",
    "        while heap:\n",
    "            entry = heap[0]     # top entry in the heap\n",
    "            endindex = entry.start_position + len(entry.word) - 1\n",
    "            if chart[endindex].back_pointer is not None:        \n",
    "                # preventry = chart[endindex]\n",
    "                preventry = chart[endindex].back_pointer\n",
    "                if entry.log_probability > preventry.log_probability:\n",
    "                    chart[endindex] = entry\n",
    "                else:\n",
    "                    continue    ## we have already found a good segmentation until endindex ##\n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "        \n",
    "        for j in range(endindex+1, len(text)):\n",
    "            newword = text[endindex+1 : j+1]\n",
    "            if newword in self.Pw:\n",
    "                newentry = Entry(newword, endindex+1, entry.log_probability+log10(self.Pw(newword), entry))\n",
    "                if newentry not in heap:\n",
    "                    heapq.heappush(heap, newentry)\n",
    "        \n",
    "        ## Get the best segmentation ##\n",
    "        finalindex = len(text)\n",
    "        finalentry = chart[finalindex]\n",
    "        segmentation = []\n",
    "        while finalentry:\n",
    "            segmentation.append(finalentry.word)\n",
    "            finalentry = finalentry.back_pointer\n",
    "\n",
    "        return segmentation\n",
    "\n",
    "    def Pwords(self, words): \n",
    "        \"The Naive Bayes probability of a sequence of words.\"\n",
    "        return product(self.Pw(w) for w in words)\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./(N*1100**len(k)))\n",
    "    def __call__(self, key): \n",
    "        if key in self: return (self[key]+1)/(self.N)  \n",
    "        else: return self.missingfn(key, self.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenter Algorithm with Unigram and Bigram Probability\n",
    "\n",
    "The code below is the initial iterative Segmenter algorithm combining unigram and bigram approach.\n",
    "\n",
    "In an attempt to weight the predictive power of the unigram or bigram sequences being chosen, we implemented Jelinek-Mercer smoothing through the linear interpolation form. By varying the lambda parameter (the final one chosen was $\\lambda = 0.9$) we obtained a score of $0.92$ on the `dev` dataset.\n",
    "\n",
    "Considering probability for long unseen words, we performed Laplace smoothing on the unigram probability model by modifying `self.missingfn` in `class Pdist`, and the overall dev score of the unigram approach reached 0.92. The code for unigram's `Pdist` is included at the bottom of the code segment, where `self.missingfn` is changed from:\n",
    "\n",
    "```\n",
    "self.missingfn = missingfn or (lambda k, N: 1./ N)\n",
    "```\n",
    "to\n",
    "```\n",
    "self.missingfn = missingfn or (lambda k, N: (1+1)/(N*1000**len(k)))\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "        segmentation = [ w for w in text ] # segment each char into a word\n",
    "        return segmentation\n",
    "\n",
    "    def segment_iter(self, text):\n",
    "        heap, chart = [], {}\n",
    "        \n",
    "        word = \"\"\n",
    "        for w in range(0,len(text)):\n",
    "            word += text[w]\n",
    "            chart[w] = self.Entry(None,None,None,None)\n",
    "            if (word in self.Pw_uni or len(word) < 6):\n",
    "                heapq.heappush(heap, self.Entry(word, 0, 0.9*log10(self.Pw_uni(word)), None))\n",
    "            \n",
    "        \n",
    "        while (len(heap)!=0):\n",
    "            top_entry = heapq.heappop(heap)\n",
    "            endIndex = top_entry.start_pos + len(top_entry.word)-1\n",
    "            if (chart[endIndex].back_pointer is not None):\n",
    "                prevEntry = chart[endIndex].back_pointer\n",
    "                if(top_entry.log_prob > prevEntry.log_prob):\n",
    "                    chart[endIndex] = top_entry\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                chart[endIndex] = top_entry\n",
    "\n",
    "            newWord = \"\"\n",
    "            for n in range(endIndex+1,len(text)):\n",
    "                newWord += text[n]\n",
    "                if (newWord in self.Pw_uni or len(newWord) < 6):\n",
    "                    wordPair = top_entry.word+\" \"+newWord\n",
    "                    if (wordPair in self.Pw_bi and top_entry.word in self.Pw_uni):\n",
    "                        newEntry = self.Entry(newWord, endIndex+1, top_entry.log_prob + 0.1*log10(self.Pw_bi(wordPair)/self.Pw_uni(newWord)), top_entry)\n",
    "                    else:\n",
    "                        newEntry = self.Entry(newWord, endIndex+1, top_entry.log_prob+log10(self.Pw_uni(newWord)), top_entry)\n",
    "                    if (newEntry not in heap):\n",
    "                        heapq.heappush(heap, newEntry)\n",
    "        \n",
    "        finalIndex = len(text)-1\n",
    "        finalEntry = chart[finalIndex]\n",
    "        best_segment = []\n",
    "        while (finalEntry is not None):\n",
    "            best_segment.insert(0,finalEntry.word)\n",
    "            finalEntry = finalEntry.back_pointer\n",
    "        return best_segment\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: (1+1)/(N*1000**len(k)))\n",
    "    def __call__(self, key): \n",
    "        if key in self: return (self[key]+1)/(self.N)  \n",
    "        else: return self.missingfn(key, self.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白国良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "Pw_uni = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "Pw_bi = Pdist(data=datafile(\"../data/count_2w.txt\"))\n",
    "segmenter = Segment(Pw_uni,Pw_bi)\n",
    "output_full = []\n",
    "with open('../data/input/dev.txt') as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment_iter(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?\n",
    "\n",
    "We have approached multiple ways in implementing the unigram and bigram models. For unigram we first implemented the baseline algorithm, but ended up with `dev.score` of $0.88$ onl. So we further enhanced accuracy to $0.93$ by performing additive and laplace smoothing on the probability distribution of the counts of words in the count files.\n",
    "\n",
    "We then moved on to implement the bigram model combining our initial unigram iterative algorithm, which gave us a `dev.score` of $0.81$. Then we attempted to improve accuracy by using backoff smoothing to prioritize unigram probabilities over grouped tokens, which boost the score to $0.90$. \n",
    "\n",
    "Eventually, we attempted in performing linear interpolation on the models, where we implemented Jelinek-Mercer smoothing by testing various lambda parameter. The final `dev.score` we got with JM smoothing is $0.92$."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d08f6de6c3bba0279a1f57ba8cba05730364c1b881f78a284190d1f2f656e41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 32-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
