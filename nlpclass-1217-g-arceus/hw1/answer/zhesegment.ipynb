{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-c957035a3c89>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-c957035a3c89>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from ../default import *\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from zhsegment import *\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "        segmentation = [ w for w in text ] # segment each char into a word\n",
    "        return segmentation\n",
    "\n",
    "    def Pwords(self, words): \n",
    "        \"The Naive Bayes probability of a sequence of words.\"\n",
    "        return product(self.Pw(w) for w in words)\n",
    "\n",
    "#### Support functions (p. 224)\n",
    "\n",
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    return reduce(operator.mul, nums, 1)\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key): \n",
    "        if key in self: return self[key]/self.N  \n",
    "        else: return self.missingfn(key, self.N)\n",
    "\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    with open(name) as fh:\n",
    "        for line in fh:\n",
    "            (key, value) = line.split(sep)\n",
    "            yield (key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Entry:\n",
    "    def __init__(self, word, start_position, log_probability, back_pointer):\n",
    "        self.word = word\n",
    "        self.start_position = start_position\n",
    "        self.log_probability = log_probability\n",
    "        self.back_pointer = back_pointer\n",
    "    def __eq__ (self, entry):\n",
    "        if( self.word == entry.word) and (self.start_position == entry.start_position) and (self.log_probability == entry.log_probability):\n",
    "            return True\n",
    "        if not isinstance(entry, type(self)):\n",
    "            return False\n",
    "    def __lt__(self, entry):   # for < instances in heap\n",
    "        return self.log_probability > entry.log_probability\n",
    "    def __gt__(self, entry):   # for > instances in heap\n",
    "        return self.log_probability <= entry.log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram approach\n",
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    def segment(self, text):\n",
    "        ## Initialize the heap, seg, chart array ##\n",
    "        heap, segmentation = [], []\n",
    "        chart = {}\n",
    "        # for each word that matches input at pos 0, insert entry into heap\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            word = text[0:i+1]\n",
    "            chart[i] = Entry(None, None, None, None)\n",
    "            if word in self.Pw or len(word) < 2:\n",
    "#                 print(\"forloop1\")\n",
    "                heapq.heappush(heap, Entry(word, 0, log10(self.Pw(word)), None))    ## heapq.heappush pushes the entry item to the heap queue\n",
    "        \n",
    "        ## Iteratively fill in chart[i] for all i ##\n",
    "        while len(heap) != 0:\n",
    "#             print(\"whileloop1\")\n",
    "            entry = heapq.heappop(heap)     # top entry in the heap\n",
    "            endindex = entry.start_position + len(entry.word) - 1\n",
    "            if chart[endindex].back_pointer is not None:        \n",
    "#                 preventry = chart[endindex]\n",
    "                preventry = chart[endindex].back_pointer\n",
    "                if entry.log_probability > preventry.log_probability:\n",
    "                    chart[endindex] = entry\n",
    "                else:\n",
    "                    continue    ## we have already found a good segmentation until endindex ##\n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "                \n",
    "            for j in range(endindex+1, len(text)):\n",
    "#                 print(\"forloop2\")\n",
    "                newword = text[endindex+1 : j+1]\n",
    "                if newword in self.Pw or len(newword) < 2:\n",
    "                    newentry = Entry(newword, endindex+1, entry.log_probability+log10(self.Pw(newword)), entry)\n",
    "                    if newentry not in heap:\n",
    "                        heapq.heappush(heap, newentry)\n",
    "        \n",
    "        ## Get the best segmentation ##\n",
    "        finalindex = len(text)\n",
    "        finalentry = chart[finalindex-1]\n",
    "        while finalentry is not None:\n",
    "            segmentation.insert(0, finalentry.word)\n",
    "            finalentry = finalentry.back_pointer\n",
    "\n",
    "        return segmentation\n",
    "\n",
    "    def Pwords(self, words): \n",
    "        \"The Naive Bayes probability of a sequence of words.\"\n",
    "        return product(self.Pw(w) for w in words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from zhsegment import *\n",
    "class Entry:\n",
    "    def __init__(self, word, start_position, log_probability, back_pointer):\n",
    "        self.word = word\n",
    "        self.start_position = start_position\n",
    "        self.log_probability = log_probability\n",
    "        self.back_pointer = back_pointer\n",
    "    def __eq__ (self, entry):\n",
    "        if( self.word == entry.word) and (self.start_position == entry.start_position) and (self.log_probability == entry.log_probability):\n",
    "            return True\n",
    "        if not isinstance(entry, type(self)):\n",
    "            return False\n",
    "        else:\n",
    "            return False\n",
    "    def __lt__(self, entry):   # for < instances in heap\n",
    "        return self.log_probability > entry.log_probability\n",
    "    def __gt__(self, entry):   # for > instances in heap\n",
    "        return self.log_probability <= entry.log_probability\n",
    "    \n",
    "# Bigram approach --> haven't modified yet\n",
    "class Segment:\n",
    "\n",
    "    def __init__(self, bigram_Pw, unigram_Pw):\n",
    "        self.bigram_Pw = bigram_Pw\n",
    "        self.unigram_Pw = unigram_Pw\n",
    "\n",
    "    def segment(self, text):\n",
    "        ## Initialize the heap, seg, chart array ##\n",
    "        heap, segmentation = [], []\n",
    "        chart = {}\n",
    "        # for each word that matches input at pos 0, insert entry into heap\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            word = text[0:i+1]\n",
    "            chart[i] = Entry(None, None, None, None)\n",
    "            if word in self.unigram_Pw or len(word) < 10:\n",
    "#                 print(\"forloop1\")\n",
    "                heapq.heappush(heap, Entry(word, 0, log10(self.unigram_Pw(word)), None))    ## heapq.heappush pushes the entry item to the heap queue\n",
    "        \n",
    "        ## Iteratively fill in chart[i] for all i ##\n",
    "        while len(heap) != 0:\n",
    "#             print(\"whileloop1\")\n",
    "            entry = heapq.heappop(heap)     # top entry in the heap\n",
    "            endindex = entry.start_position + len(entry.word) - 1\n",
    "            if chart[endindex].back_pointer is not None:        \n",
    "#                 preventry = chart[endindex]\n",
    "                preventry = chart[endindex].back_pointer\n",
    "                if entry.log_probability > preventry.log_probability:\n",
    "                    chart[endindex] = entry\n",
    "                else:\n",
    "                    continue    ## we have already found a good segmentation until endindex ##\n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "                \n",
    "            for j in range(endindex+1, len(text)):\n",
    "#                 print(\"forloop2\")\n",
    "                newword = text[endindex+1 : j+1]\n",
    "                \n",
    "#                 print(entry.word+newword)\n",
    "#                 print(entry.word+\" \"+newword)\n",
    "                if newword in self.unigram_Pw or len(newword) < 10:\n",
    "                    word_pairs = entry.word + newword\n",
    "                    if word_pairs in self.bigram_Pw and entry.word in self.unigram_Pw:\n",
    "                        newentry = Entry(newword, endindex+1, entry.log_probability+log10(self.bigram_Pw(word_pairs)/self.unigram_Pw(newword)), entry)\n",
    "                    else:\n",
    "#                         newentry = Entry(newword, endindex+1, entry.log_probability+log10(self.unigram_Pw(newword)), entry)\n",
    "                        newentry = Entry(newword, endindex+1, entry.log_probability+log10(self.unigram_Pw(newword)), entry)\n",
    "                    \n",
    "                    if newentry not in heap:\n",
    "                        heapq.heappush(heap, newentry)\n",
    "        \n",
    "        ## Get the best segmentation ##\n",
    "        finalindex = len(text)\n",
    "        finalentry = chart[finalindex-1]\n",
    "        while finalentry is not None:\n",
    "            segmentation.insert(0, finalentry.word)\n",
    "            finalentry = finalentry.back_pointer\n",
    "\n",
    "        return segmentation\n",
    "\n",
    "    def Pwords(self, words): \n",
    "        \"The Naive Bayes probability of a sequence of words.\"\n",
    "        return product(self.Pw(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram_Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "bigram_Pw = Pdist(data=datafile(\"../data/count_2w.txt\"))\n",
    "segmenter = Segment(bigram_Pw, unigram_Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d08f6de6c3bba0279a1f57ba8cba05730364c1b881f78a284190d1f2f656e41"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
