{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunker: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:02<00:00, 459.66it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('data', 'train.txt.gz'), os.path.join('data', 'chunker'), '.tar')\n",
    "decoder_output = chunker.decode('data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11672 phrases; correct: 8568.\n",
      "accuracy:  84.35%; (non-O)\n",
      "accuracy:  85.65%; precision:  73.41%; recall:  72.02%; FB1:  72.71\n",
      "             ADJP: precision:  36.49%; recall:  11.95%; FB1:  18.00  74\n",
      "             ADVP: precision:  71.36%; recall:  39.45%; FB1:  50.81  220\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  70.33%; recall:  76.80%; FB1:  73.42  6811\n",
      "               PP: precision:  92.40%; recall:  87.14%; FB1:  89.69  2302\n",
      "              PRT: precision:  65.00%; recall:  57.78%; FB1:  61.18  40\n",
      "             SBAR: precision:  84.62%; recall:  41.77%; FB1:  55.93  117\n",
      "               VP: precision:  63.66%; recall:  58.25%; FB1:  60.83  2108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73.40644276901988, 72.02420981842637, 72.70875763747455)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Implementing Character Level Representation\n",
    "\n",
    "### Create a character level representation of word\n",
    "1. Create a one-hot vector v1 for the first character of the word.\n",
    "2. Create a vector v2 where the index of a character has the count of that character in the word.\n",
    "3. Create a one-hot vector v3 for the last character of the word.\n",
    "\n",
    "With the conditions given, we can generate and store the character level representation for each word in the `torch.tensor` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunker import *\n",
    "import os, string\n",
    "import torch\n",
    "\n",
    "def char_rep(seq, to_ix):\n",
    "    chars_vec = []\n",
    "    size = len(string.printable)\n",
    "    for word in seq:\n",
    "        v1 = torch.zeros(size)\n",
    "        v2 = torch.zeros(size)\n",
    "        v3 = torch.zeros(size)\n",
    "        if len(word) > 0:\n",
    "            v1[to_ix[word[0]]] += 1\n",
    "            for c in word[1:-1]:\n",
    "                v2[to_ix[c]] += 1\n",
    "            v3[to_ix[word[-1]]] += 1\n",
    "        chars_vec.append(torch.cat((v1, v2, v3), 0))\n",
    "    return torch.tensor(torch.stack(chars_vec),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Second RNN Implementation\n",
    "We got a dev score of 76.7605 with the implemented baseline method. \n",
    "\n",
    "To better leverage the semi-character representation, a separate RNN is dedicated to learning from only the character-level data.\n",
    "Here, the semi-character representation remains the same as previously defined, but is instead passed into a LSTM layer which generates the hidden state output.\n",
    "This hidden state output is then concatenated with the word embedding representation rather than the raw semi-character representation to be passed forward into the layers from the baseline model.\n",
    "\n",
    "We define a second RNN model that takes the input as character level representation, and use the hidden layers to concatenate the word embiddings to create new input for the original chunker RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTMTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_hidden_dim, vocab_size, tagset_size):\n",
    "        torch.manual_seed(1)\n",
    "        super(CharLSTMTaggerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.char_lstm = nn.LSTM(300, char_hidden_dim, bidirectional = False)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim+char_hidden_dim, hidden_dim, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, char_rep):\n",
    "        char_lstm_out,_ = self.char_lstm(char_rep.view(len(char_rep), 1, -1))\n",
    "        char_lstm_out = char_lstm_out.reshape(len(char_rep),self.char_hidden_dim)\n",
    "        # print(char_lstm_out.shape)\n",
    "        # print(self.word_embeddings(sentence).shape)\n",
    "        embeds = torch.cat([self.word_embeddings(sentence),char_lstm_out],1)\n",
    "        # print(embeds.shape)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the class `CharLSTMTagger` we initialize the CharLSTM model, and train the second RNN using the stochastic gradient edscent and a learning rate of 0.01 as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sself.model = CharLSTMTaggerModel(self.embedding_dim, self.hidden_dim, self.char_hidden, len(self.word_to_ix), len(self.tag_to_ix))\n",
    "self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "def train(self):\n",
    "    loss_function = nn.NLLLoss()\n",
    "\n",
    "    self.model.train()\n",
    "    loss = float(\"inf\")\n",
    "    for epoch in range(self.epochs):\n",
    "        for sentence, tags in tqdm.tqdm(self.training_data):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "            char_in = semi_char(sentence)\n",
    "            targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "            \n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = self.model(sentence_in, char_in)\n",
    "            \n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if epoch == self.epochs-1:\n",
    "            epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "        else:\n",
    "            epoch_str = str(epoch)\n",
    "        savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "        print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    'unk': self.unk,\n",
    "                    'word_to_ix': self.word_to_ix,\n",
    "                    'tag_to_ix': self.tag_to_ix,\n",
    "                    'ix_to_tag': self.ix_to_tag,\n",
    "                }, savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementating the GRU Model Combining the Character Level Representations of Words\n",
    "\n",
    "As we discussed in lecture the LSTM and GRU model have very similar architectures with the consensus being start with LSTM, but if you need quicker computations switch to GRU.\n",
    "To experiment with this and whether it changes the dev-out score, we switched the LSTM layer which takes the word embedding and the hidden state output for the semi-character RNN to a GRU layer from the ``Pytorch`` Library.\n",
    "\n",
    "Below is the `CharGRUTaggerModel` class, where we took word embeddings as inputs and output hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharGRUTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_hidden_dim, vocab_size, tagset_size):\n",
    "        torch.manual_seed(1)\n",
    "        super(CharGRUTaggerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # Still use an LSTM network to learn from the semi-character representation\n",
    "        self.char_lstm = nn.LSTM(300, char_hidden_dim, bidirectional = False)\n",
    "\n",
    "        # The GRU takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.gru = nn.GRU(embedding_dim+char_hidden_dim, hidden_dim, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, char_rep):\n",
    "        char_lstm_out,_ = self.char_lstm(char_rep.view(len(char_rep), 1, -1))\n",
    "        char_lstm_out = char_lstm_out.reshape(len(char_rep),self.char_hidden_dim)\n",
    "\n",
    "        #Concatenate the word embeddings with the hidden state output.\n",
    "        embeds = torch.cat([self.word_embeddings(sentence),char_lstm_out],1)\n",
    "        \n",
    "        #CHANGED: GRU output instead of LSTM.\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model = CharGRUTaggerModel(self.embedding_dim, self.hidden_dim, self.char_hidden, len(self.word_to_ix), len(self.tag_to_ix))\n",
    "self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We got a dev score of 77.1090 implementing the baseline method. \n",
    "\n",
    "We got a dev score of 76.7605 implementing LSTM second RNN (option 2) method. \n",
    "\n",
    "We got a dev score of 77.4384 implementing GRU model method. \n",
    "\n",
    "\n",
    "## Analysis\n",
    "### For the Baseline Method\n",
    "We got a dev score of 77.1090 with the implemented baseline method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTaggerModel(\n",
      "  (word_embeddings): Embedding(9675, 128)\n",
      "  (lstm): LSTM(428, 64)\n",
      "  (hidden2tag): Linear(in_features=64, out_features=22, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1027 [00:00<?, ?it/s]/home/felicity/nlpclass-1217-g-arceus/hw3/answer/chunker.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(torch.stack(chars_vec),dtype=torch.long)\n",
      "100%|██████████| 1027/1027 [00:04<00:00, 213.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11930 phrases; correct: 9186.\n",
      "accuracy:  86.95%; (non-O)\n",
      "accuracy:  87.91%; precision:  77.00%; recall:  77.22%; FB1:  77.11\n",
      "             ADJP: precision:  45.56%; recall:  18.14%; FB1:  25.95  90\n",
      "             ADVP: precision:  68.38%; recall:  46.73%; FB1:  55.52  272\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  75.38%; recall:  80.52%; FB1:  77.87  6662\n",
      "               PP: precision:  91.37%; recall:  88.45%; FB1:  89.88  2363\n",
      "              PRT: precision:  70.27%; recall:  57.78%; FB1:  63.41  37\n",
      "             SBAR: precision:  86.29%; recall:  45.15%; FB1:  59.28  124\n",
      "               VP: precision:  69.06%; recall:  71.40%; FB1:  70.21  2382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76.99916177703268, 77.21923335574982, 77.10904054394359)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('../data', 'train.txt.gz'), os.path.join('../data', 'chunker'), '.tar')\n",
    "print(chunker.model)\n",
    "decoder_output = chunker.decode('../data/input/dev.txt')\n",
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('../data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis\n",
    "\n",
    "Between the suggested options, the baseline provides the better performance.\n",
    "Yet, by changing the LSTM layer to a GRU layer gives a small score increase of approximately 0.3.\n",
    "This is an interesting result due to the fact that a GRU layer is usually considered a more simple approach than an LSTM layer.\n",
    "This could perhaps be indicative that the LSTM-based model tends to overfit a bit more.\n",
    "\n",
    "We could also tweak the values for the learning rate and the number of `epochs` when training the models to see if the task performance will improve for implementing the second RNN.\n",
    "\n",
    " However, running these models took a very long time to do so, which reflects training them are quite expensive time-wise. With the same simple dataset, GRU model processed had a slightly better performance considering its properties of less training parameters and simple architectures. LSTM on the otherhand is more complex to implement, but would work better than GRU if the training data are larger. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89a1246eed7aeb4193801674f3e8466861a023fbbc3e27dec9ccfbbab1550f8e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
