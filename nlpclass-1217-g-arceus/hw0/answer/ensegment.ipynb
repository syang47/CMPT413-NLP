{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modified Ensegment Program - Team Arceus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Documentation\r\n",
    "\r\n",
    "### The main modification we did\r\n",
    "\r\n",
    "After reading through the Norvig textbook chapter 14's section on n-grams and word segmentation, we noticed one major ambiguity between the default code and the sample snippet present in the textbook:\r\n",
    "\r\n",
    "The default code has the helper function that initializes `missingfn` does not take into account:\r\n",
    "- Size of candidate word (k)\r\n",
    "- Avoid longest word functions\r\n",
    "```\r\n",
    "self.missingfn = missingfn or (lambda k, N: 1.0 / N)\r\n",
    "```\r\n",
    "So we just modified it to:\r\n",
    "```\r\n",
    "self.missingfn = missingfn or (lambda k, N: 10.0 / (N * 10 ** len(k)))\r\n",
    "```\r\n",
    "### The overall segmentation results of the small edit are:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from ensegment import *\r\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\r\n",
    "segmenter = Segment(Pw)\r\n",
    "with open(\"../data/input/dev.txt\") as f:\r\n",
    "    for line in f:\r\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "\n",
    "With the default unseen word probability of 1/N, we modified the probability for unseen word to avoid having high probability for very long words. For example, the default solution for \"dev.txt\" still consists of the following unsegmented words: \n",
    "<br>\n",
    "- \"unclimatechangebody\"\n",
    "- \"mentionyourfaves\"\n",
    "- \"30secondstoearth\"\n",
    "- \"ratesoughttogodown\"\n",
    "- \"nowthatcherisdead\"\n",
    "\n",
    "\n",
    "The default solution gives a F-score of 0.82.\n",
    "We initially changed the probability from 1/N to 10/N, which did not improve the segmentation accuracy and lowered the score to 0.78. We further decrease the probability by a factor of 5 for each letter in the candidate word, which improved the score to 0.97, but still has the faulty word \"30secondstoearth\".\n",
    "\n",
    "When the program attempts to segment the word \"30secondstoearth\", the probability of P(\"30 seconds to earth\") will be lower than the P(\"30secondstoearth) because \"30\" cannot be found in the corpus file \"count_lw.txt\".\n",
    "\n",
    "Therefore we further decrease the probability of unseen words by a factor 0f 10 for each word, which improved F-score to 1.0, and all words from \"dev.txt\" seem to be segmented properly."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}